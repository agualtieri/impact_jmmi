---
title: "JMMI Scripts"
output: html_document

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(tidyverse)
library(reshape)
library(cluster)
library(butteR)

# load dataset
data <- read.csv("./inputs/test_dataset.csv", stringsAsFactors = FALSE)
tool <- read.csv("./inputs/test_tool.csv", stringsAsFactors = FALSE)

month1 <- read.csv("./inputs/test_dataset_m1.csv", stringsAsFactors = FALSE)
month2 <- read.csv("./inputs/test_dataset_m2.csv", stringsAsFactors = FALSE)


# load sources
source("./R/find_outliers.R")
source("./R/item_boxplots.R")
source("./R/descriptive_stats.R")
source("./R/minimum_standards.R")
source("./R/data_falsification.R")

```

# Introduction

This (living) document will provide users with a list of scripts useful for conducting basic cleaning and analysis of JMMI data. This is more a repository of code and scripts rather than a full working package. The document is divided into two sections: Cleaning and Analysis. 

The first section, cleaning, will focus on a few packages/chunks of code, the user can employ during the data cleaning phase. These include, duplicates correction, outliers detection, min and max analysis, and enumerator falsification analysis. 

The second section, analysis, focuses on the calculations needed to compute aggregated median prices (following the median-of-medians methodology), SMEB and MEB aggregations, and trend analysis.

For further inquires or if you would like to have different type of checks included, please reach out to Chris (chris.paci@reach-initiative.org) and/or Alberto (alberto.gualtieri@impact-initiatives.org).

```{r test dataset}
# Load test data
head(data, 3)
```
# Data Cleaning

### Minimum Standards

The minimum standards function allows you to quickly check how many markets per area of interest have been collected by the enumerators. The function returns a table with the number of market collected aggregated by your variable of interest (it can be district, or governorate).

```{r minimum standards, message = FALSE, warning = FALSE}
minium_standards(data, district, market)

```

### Outliers detection

The function that checks for outliers is called "find_outliers". You can find it inside the "R" folder within the project. The function checks for normal outliers and also log distributed outliers and the output is a table with the outlier value, the index number, and the variable name.

```{r outliers detection}
outliers_table <- find_outliers(data)

# Add uuid and location to the output
outliers_table <- outliers_table %>% mutate(uuid= data[.$index,"uuid",drop=T],
                                            area = data[.$index,"district",drop=T])

head(outliers_table, 3)

```



### Plotting Item prices using boxplots

The function below will help with visualizing the prices using boxplots. You can find it inside the "R" folder. Box plots visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages. 

Hot to read a boxplot: box plots show the five-number summary of a set of data: including the minimum score, first (lower) quartile, median, third (upper) quartile, and maximum score.

If you want to read more about boxplot you can find a guide [here](https://www.simplypsychology.org/boxplots.html)


```{r boxplots, message = FALSE, warning = FALSE}
item_boxplots(data, "district", starts_with("item"))

```

### Descriptive statistics

If you want to have a quick look at general descriptive statistics, the function "descriptive_stats" will show you min, max, and median values, as well as, first and second quartile values. The output is a table that can be saved as a dataframe.

```{r descripive statistics, message=FALSE, warning=FALSE}

descriptive_stats(data, "district", starts_with("item"))

```

### Falsification test / Silhouette Analysis

A "Silhouette Analysis" is based on the "gower" distance between surveys which is a metric that measures the dissimilarities between two items with mixed numeric and non-numeric data. This method checks for anomalies assuming the datast is clustered around the enumerator ID. A silhouette value of 1 indicates that the entries of the cluster are very similar to each other, while a value close to 0 or negative indicates the opposite. Values close to 1 should raise a flag as it means that enumerators are filling surveys very similar to one another.

For more information on this type of falsification test you can read [here](https://en.wikipedia.org/wiki/Silhouette_(clustering)), and [here](https://medium.com/@rumman1988/clustering-categorical-and-numerical-datatype-using-gower-distance-ab89b3aa90d9).

```{r silhouette analysis, message = FALSE, warning = FALSE}
# To run the function you will need the dataset, the kobo tool, the enumerator_id or enumerator_name colun, and and admin column

calculateEnumeratorSimilarity(data, tool, "enum_id", "governorate")

```
# Data Analysis

### Median aggregation

The first step to calculate the SMEB value is to aggregate prices using the median-of-medians methodology. This methodology ensures a balanced aggregation reducing the variance created by having markets largely more surveyed compared to others.

```{r median aggregation, message = FALSE, warning = FALSE}

# Aggregation by governorate, district, and market
median_items <- data %>%                                                                              
                select(-uuid, -enum_id, -index) %>%                             # select only the items and aggregation columns
                group_by(governorate, district, market) %>%                     # group by what you want
                summarise_all(funs(median(., na.rm = TRUE)))                    # aggregate by median

print(head(median_items, 3))                                             

# Aggregation by governorate and district
median_items_district <- data %>%                                                                            
                        select(-uuid, -enum_id, -index, -market) %>% group_by(governorate, district) %>%     
                        summarise_all(funs(median(., na.rm = TRUE)))  

print(head(median_items_district, 3))                                     
```

### SMEB calculation

Below we show a few lines of code you can use to quickly calculate the SMEB value after having calculated the median price per item. The calculation will be different based on the weights and the items included in the SMEB. If you nee to split the calculation across multiple SMEBs you can split the code.

```{r smeb calculation, message = FALSE, warning = FALSE}
# This piece of code will calculate the smeb value for each item

smeb <- median_items_district %>% group_by(district) %>%          # the group_by can be changed based on the aggregation level you desire
                                  mutate(
                                  smeb_item1 = item1 * 0.5,
                                  smeb_item2 = item2 * 0.5,
                                  smeb_item3 = item3 * 0.5,
                                  smeb_item4 = item4 * 0.5,
                                  smeb_item5 = item5 * 0.5,
                                  smeb_item6 = item6 * 0.5
                                  ) 

print(head(smeb, 3))

# If NAs are present the following code will input the median value of the district of govenorate of choice to fill the blank
smeb_no_NAs <- smeb %>% group_by(district) %>%  # the group_by can be changed based on what median value you want to input
                        mutate(
                              smeb_item1= ifelse(is.na(smeb_item1), median(smeb_item1, na.rm=T), smeb_item1),
                              smeb_item2= ifelse(is.na(smeb_item2), median(smeb_item2, na.rm=T), smeb_item2),
                              smeb_item3= ifelse(is.na(smeb_item3), median(smeb_item3, na.rm=T), smeb_item3),
                              smeb_item4= ifelse(is.na(smeb_item4), median(smeb_item4, na.rm=T), smeb_item4),
                              smeb_item5= ifelse(is.na(smeb_item5), median(smeb_item5, na.rm=T), smeb_item5),
                              smeb_item6= ifelse(is.na(smeb_item6), median(smeb_item6, na.rm=T), smeb_item6)
                              ) %>% select(governorate, district, starts_with("smeb"))

print(head(smeb_no_NAs, 3))

```

### Trend analysis


```{r trend analysis, message = FALSE, warning = FALSE}
# After loading the datset from two consecutive months, we rbind them together
data2 <- rbind(month1, month2) 

# We aggregate the median dataset to the desired level (district in this case)
median_items_district <- data2 %>%                                                                            
                         select(-uuid, -enum_id, -index, -market) %>% group_by(month, governorate, district) %>%     
                         summarise_all(funs(median(., na.rm = TRUE)))  

# Calculate the smeb
smeb <- median_items_district %>% group_by(district) %>%      
                                  mutate(
                                  smeb_item1 = item1 * 0.5,
                                  smeb_item2 = item2 * 0.5,
                                  smeb_item3 = item3 * 0.5,
                                  smeb_item4 = item4 * 0.5,
                                  smeb_item5 = item5 * 0.5,
                                  smeb_item6 = item6 * 0.5
                                  ) 

# Create the comparison column
smeb$month_date<- lubridate::month(smeb$month)

# Run the percentag change formula taken from the butteR package

# If needed download the latest version of th butteR package and load the library
# devtools::install_github("zackarno/butteR")
# library(butteR)

pct_change <- smeb %>% pct_change_by_groups_all_numerics(group_var = "district", time_id = "month_date")

print(head(pct_change, 3))
```
